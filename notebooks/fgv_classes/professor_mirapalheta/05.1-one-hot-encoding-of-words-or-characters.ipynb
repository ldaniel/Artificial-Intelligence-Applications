{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"05.1-one-hot-encoding-of-words-or-characters.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"LJQZBcmeVqfG","colab_type":"code","colab":{},"outputId":"bcb6296b-cffa-46e4-e2cd-d74330207bb2"},"source":["import keras\n","keras.__version__"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'2.0.8'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"GG4itQJZVqfL","colab_type":"text"},"source":["# One-hot encoding of words or characters\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TtMFzCdoV2u-","colab_type":"text"},"source":["A técnica de one-hot encoding consiste em associar um número inteiro para cada palavra e em seguida tornar este número em uma sequência de N-1 zeros e de 1 número um. Neste caso N é o tamanho do vocabulário. Esta técnica pode ser aplicada também aos caracteres de cada palavra ao invés das palavras em sí. A seguir apresentamos dois exemplos, um para palavras e outro para caracteres."]},{"cell_type":"code","metadata":{"id":"OU1wITS9VqfM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"outputId":"325134d7-e15e-4cf8-a30f-2aab3b994599","executionInfo":{"status":"ok","timestamp":1578591285743,"user_tz":180,"elapsed":614,"user":{"displayName":"Gustavo Mirapalheta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWGB61oq68YgCpmsgzde-siuuqzysncEyJ80ex=s64","userId":"00687033162702398655"}}},"source":["import numpy as np\n","\n","# Estes são os nossos dados: duas amostras, uma entrada por amostra\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","# Primeiro, criamos um índice para todos os tokens nos dados\n","token_index = {}\n","for sample in samples:\n","    # Neste exemplo as amostras foram tokenizadas com o método 'split'.\n","    for word in sample.split():\n","        if word not in token_index:\n","            # Designamos um índice único para cada palavra\n","            token_index[word] = len(token_index) + 1\n","            # Observe que o índice 0 não foi atribuido a nenhuma palavra\n","\n","# Em seguida, vetorizamos nossas amostras.\n","# Serão utilizados apenas as primeiras `max_length` palavras de cada amostra.\n","max_length = 10\n","\n","# Armazenamos os resultados em um numpy array:\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n","for i, sample in enumerate(samples):\n","    for j, word in list(enumerate(sample.split()))[:max_length]:\n","        index = token_index.get(word)\n","        results[i, j, index] = 1.\n","\n","results"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"wsqitBUkVqfO","colab_type":"text"},"source":["Exemplo com caracteres:"]},{"cell_type":"code","metadata":{"id":"ggKMF4hCVqfP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"8f7f821a-12a3-4889-9e09-6d45a7e20c5e","executionInfo":{"status":"ok","timestamp":1578591326538,"user_tz":180,"elapsed":624,"user":{"displayName":"Gustavo Mirapalheta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWGB61oq68YgCpmsgzde-siuuqzysncEyJ80ex=s64","userId":"00687033162702398655"}}},"source":["import string\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","characters = string.printable  # All printable ASCII characters.\n","token_index = dict(zip(characters, range(1, len(characters) + 1)))\n","\n","max_length = 50\n","results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n","for i, sample in enumerate(samples):\n","    for j, character in enumerate(sample[:max_length]):\n","        index = token_index.get(character)\n","        results[i, j, index] = 1.\n","\n","results"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"xaYP_jB9VqfR","colab_type":"text"},"source":["Observe que o Keras possui utilitários para executar o one-hot encoding a partir de dados \"raw\" tanto com palavras quanto com caracteres. Estas funções executam várias tarefas importantes tais como extração de caracteres especiais de strings ou inclusão de apenas as N palavras mais comuns no seu conjunto de dados (uma restrição comum para evitar lidar com espaços vetoriais de entrada muito grandes). O exemplo a seguir executa o one-hot encoding de palavras através dos recursos da Keras:"]},{"cell_type":"code","metadata":{"id":"bm2ziH5AVqfS","colab_type":"code","colab":{},"outputId":"1f52084a-3687-44d7-dfd3-3d13be340407"},"source":["from keras.preprocessing.text import Tokenizer\n","\n","samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","# Criamos um tokenizer, configurado para levar em consideração\n","# apenas as 1000 palavras mais comuns.\n","tokenizer = Tokenizer(num_words=1000)\n","# Esta etapa monte o índice de palavras\n","tokenizer.fit_on_texts(samples)\n","\n","# Aqui tornamos strings em listas de índices inteiros.\n","sequences = tokenizer.texts_to_sequences(samples)\n","\n","# É possível obter representações binárias do tipo one-hot diretamente.\n","# Outros métodos de vetorização também podem ser utilizados. \n","one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n","\n","# Aqui recuperamos os índices de palavras que foram criados \n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 9 unique tokens.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4oY07UifVqfU","colab_type":"text"},"source":["Uma variante da codificação one-hot é o chamado \"truque de hash one-hot\", que pode ser usado quando o número de tokens exclusivos no seu vocabulário é muito grande para ser explicitamente utilizado. Em vez de atribuir um índice a cada palavra e manter uma referência desses índices em um dicionário, pode-se usar um hash em vetores de tamanho fixo. Isso geralmente é feito com uma função de hash bem simples. A principal vantagem desse método é que ele acaba com a manutenção de um índice explícito de palavras, o que economiza memória e permite a codificação on-line dos dados (começando a gerar vetores de token imediatamente, antes de ver todos os dados disponíveis). A única desvantagem desse método é que ele é suscetível a \"colisões de hash\": duas palavras diferentes podem acabar com o mesmo hash e, subsequentemente, qualquer modelo de aprendizado de máquina que observe esses hashes não será capaz de diferenciar essas palavras. . A probabilidade de colisões de hash diminui quando a dimensionalidade do espaço de hash é muito maior que o número total de tokens únicos sendo hash. A seguir é apresentado um exemplo de codificação de palavras com a hashing trick."]},{"cell_type":"code","metadata":{"id":"8zVkMAMLVqfV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":281},"outputId":"63bb9d26-61f5-4286-d29e-4b7570a548c6","executionInfo":{"status":"ok","timestamp":1578592036511,"user_tz":180,"elapsed":603,"user":{"displayName":"Gustavo Mirapalheta","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAWGB61oq68YgCpmsgzde-siuuqzysncEyJ80ex=s64","userId":"00687033162702398655"}}},"source":["samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n","\n","# As palavras serão armazenadas como vetores de tamanho 1.000\n","# Observe que se você tiver aproximadamente 1.000 palavras (ou mais)\n","# você começará a ter muitas \"colisões\" de hash, o que irá diminuir \n","# a precisão deste método de codificação..\n","dimensionality = 1000\n","max_length = 10\n","\n","results = np.zeros((len(samples), max_length, dimensionality))\n","for i, sample in enumerate(samples):\n","    for j, word in list(enumerate(sample.split()))[:max_length]:\n","        # Codifica (hash) a palavra em um índice inteiro \"aleatório\" \n","        # entre 0 e 1.000\n","        index = abs(hash(word)) % dimensionality\n","        results[i, j, index] = 1.\n","\n","results"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]],\n","\n","       [[0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.],\n","        [0., 0., 0., ..., 0., 0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":4}]}]}